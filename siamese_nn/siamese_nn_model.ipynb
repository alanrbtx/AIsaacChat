{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alanbarsag/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, BertModel\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'cointegrated/rubert-tiny'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('siamese_dataset', 'r') as f:\n",
    "    text = f.readlines()\n",
    "    text = text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('siamese_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper processing from text file\n",
    "raw_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#long processing from the text file\n",
    "first_column = []\n",
    "for i in text:\n",
    "    test = i.split(',')\n",
    "    test[0] = test[0].replace('\"', '')\n",
    "    first_column.append(test[0])\n",
    "\n",
    "second_column = []\n",
    "for i in text:\n",
    "    test = i.split(',')\n",
    "    test[1] = test[1].replace('\"', '')\n",
    "    second_column.append(test[1])\n",
    "\n",
    "labels_column = []\n",
    "for i in text:\n",
    "    test = i.split(',')\n",
    "    test[2] = test[2].replace('\"', '')\n",
    "    test[2] = test[2].replace('\\n', '')\n",
    "    test[2] = int(test[2])\n",
    "    labels_column.append(test[2])\n",
    "\n",
    "df = pd.DataFrame({'first': first_column, 'second': second_column, 'labels': labels_column})\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['first'], example['second'])\n",
    "\n",
    "dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.remove_columns(['first', 'second'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super().__init__()\n",
    "        self.lambd = lambd\n",
    "    \n",
    "    def forward(self, x):\n",
    "         return self.lambd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNN, self).__init__()\n",
    "        l1_norm = lambda x: 1 - torch.abs(x[0] - x[1])\n",
    "        self.encoder = BertModel.from_pretrained(checkpoint)\n",
    "        self.merged = Lambda(l1_norm)\n",
    "        self.fc1 = torch.nn.Linear(312, 2)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        first_encoded = self.encoder(**x[0]).pooler_output\n",
    "        #print(\"First: \", first_encoded)\n",
    "        second_encoded = self.encoder(**x[1]).pooler_output\n",
    "        l1_distance = self.merged([first_encoded, second_encoded])\n",
    "        #print(l1_distance.shape)\n",
    "        fc1 = self.fc1(l1_distance)\n",
    "        fc1 = self.softmax(fc1)\n",
    "        return fc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sent_tokenized = tokenizer(first_column, max_length=20, padding='max_length', truncation=True, return_tensors='pt')\n",
    "second_sent_tokenized = tokenizer(first_column, max_length=20, padding='max_length', truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_number = len(first_sent_tokenized['input_ids'])//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sent_raw_batches = [raw_dataset['first'][16*i: 16*(i+1)] for i in range(batch_number)]\n",
    "s_sent_raw_batches = [raw_dataset['second'][16*i: 16*(i+1)] for i in range(batch_number)]\n",
    "labels_batches = [labels_column[16*i: 16*(i+1)] for i in range(batch_number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sent_tokenized_batches = [tokenizer(batch, max_length=20, padding='max_length', truncation=True, return_tensors='pt') for batch in f_sent_raw_batches]\n",
    "s_sent_tokenized_batches = [tokenizer(batch, max_length=20, padding='max_length', truncation=True, return_tensors='pt') for batch in s_sent_raw_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 13, 3, 5, 10, 2, 4, 9, 6, 1, 15, 7, 16, 0, 12, 8, 17, 11]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [i for i in range(batch_number)]\n",
    "random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sent_shuffled = []\n",
    "s_sent_shuffled = []\n",
    "labels_shuffled = []\n",
    "\n",
    "for i in idx:\n",
    "    f_sent_shuffled.append(f_sent_tokenized_batches[i])\n",
    "    s_sent_shuffled.append(s_sent_tokenized_batches[i])\n",
    "    labels_shuffled.append(labels_batches[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for s_batch, f_batch, target in zip(f_sent_tokenized_batches, s_sent_tokenized_batches, labels_batches):\n",
    "        output = model([s_batch, f_batch])\n",
    "        loss = loss_fn(output, torch.tensor(target))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7w/lj581pkd4_95yc_rm7yk7z4h0000gn/T/ipykernel_71086/535803318.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fc1 = self.softmax(fc1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0049, 0.9951],\n",
       "        [0.0061, 0.9939],\n",
       "        [0.0083, 0.9917],\n",
       "        [0.0053, 0.9947],\n",
       "        [0.0062, 0.9938],\n",
       "        [0.0063, 0.9937],\n",
       "        [0.0053, 0.9947],\n",
       "        [0.0076, 0.9924],\n",
       "        [0.0055, 0.9945],\n",
       "        [0.0042, 0.9958],\n",
       "        [0.0086, 0.9914],\n",
       "        [0.0044, 0.9956],\n",
       "        [0.0051, 0.9949],\n",
       "        [0.0043, 0.9957],\n",
       "        [0.0044, 0.9956],\n",
       "        [0.0048, 0.9952]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([f_sent_tokenized_batches[0], s_sent_tokenized_batches[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7w/lj581pkd4_95yc_rm7yk7z4h0000gn/T/ipykernel_71086/535803318.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fc1 = self.softmax(fc1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0112, 0.9888]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_sent = \"Нарисуй Альберта Эйнштейна в стиле Ван Гога\"\n",
    "s_sent = \"Нарисуй изображение\"\n",
    "\n",
    "f_sent = tokenizer(f_sent, max_length=20, padding='max_length', return_tensors='pt')\n",
    "s_sent = tokenizer(s_sent, max_length=20, padding='max_length', return_tensors='pt')\n",
    "\n",
    "model([f_sent, s_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'siamese_state')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
