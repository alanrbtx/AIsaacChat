{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, BertModel\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'cointegrated/rubert-tiny'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('siamese_dataset')\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(raw_dataset):\n",
    "    batch_number = len(raw_dataset['f_sents'])//16\n",
    "\n",
    "    f_sent_raw_batches = [raw_dataset['f_sents'][16*i: 16*(i+1)] for i in range(batch_number)]\n",
    "    s_sent_raw_batches = [raw_dataset['s_sents'][16*i: 16*(i+1)] for i in range(batch_number)]\n",
    "    labels_batches = [raw_dataset['similarity'][16*i: 16*(i+1)] for i in range(batch_number)]\n",
    "\n",
    "    f_sent_tokenized_batches = [tokenizer(batch, max_length=20, padding='max_length', truncation=True, return_tensors='pt') for batch in f_sent_raw_batches]\n",
    "    s_sent_tokenized_batches = [tokenizer(batch, max_length=20, padding='max_length', truncation=True, return_tensors='pt') for batch in s_sent_raw_batches]\n",
    "\n",
    "    idx = [i for i in range(batch_number)]\n",
    "    random.shuffle(idx)\n",
    "\n",
    "    f_sent_shuffled = []\n",
    "    s_sent_shuffled = []\n",
    "    labels_shuffled = []\n",
    "\n",
    "    for i in idx:\n",
    "        f_sent_shuffled.append(f_sent_tokenized_batches[i])\n",
    "        s_sent_shuffled.append(s_sent_tokenized_batches[i])\n",
    "        labels_shuffled.append(labels_batches[i])\n",
    "\n",
    "    return f_sent_shuffled, s_sent_shuffled, labels_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sents, s_sents, labels = preprocess_data(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super().__init__()\n",
    "        self.lambd = lambd\n",
    "    \n",
    "    def forward(self, x):\n",
    "         return self.lambd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNN, self).__init__()\n",
    "        l1_norm = lambda x: 1 - torch.abs(x[0] - x[1])\n",
    "        self.encoder = BertModel.from_pretrained(checkpoint)\n",
    "        self.merged = Lambda(l1_norm)\n",
    "        self.fc1 = torch.nn.Linear(312, 2)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        first_encoded = self.encoder(**x[0]).pooler_output\n",
    "        #print(\"First: \", first_encoded)\n",
    "        second_encoded = self.encoder(**x[1]).pooler_output\n",
    "        l1_distance = self.merged([first_encoded, second_encoded])\n",
    "        #print(l1_distance.shape)\n",
    "        fc1 = self.fc1(l1_distance)\n",
    "        fc1 = self.softmax(fc1)\n",
    "        return fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_fn, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for s_batch, f_batch, target in zip(f_sents, s_sents, labels):\n",
    "            output = model([s_batch, f_batch])\n",
    "            loss = loss_fn(output, torch.tensor(target))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, optimizer, loss_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7w/lj581pkd4_95yc_rm7yk7z4h0000gn/T/ipykernel_93752/535803318.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fc1 = self.softmax(fc1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0035, 0.9965],\n",
       "        [0.0063, 0.9937],\n",
       "        [0.0043, 0.9957],\n",
       "        [0.0055, 0.9945],\n",
       "        [0.0041, 0.9959],\n",
       "        [0.0041, 0.9959],\n",
       "        [0.0043, 0.9957],\n",
       "        [0.0041, 0.9959],\n",
       "        [0.0037, 0.9963],\n",
       "        [0.0048, 0.9952],\n",
       "        [0.0036, 0.9964],\n",
       "        [0.0044, 0.9956],\n",
       "        [0.0036, 0.9964],\n",
       "        [0.0063, 0.9937],\n",
       "        [0.0035, 0.9965],\n",
       "        [0.0038, 0.9962]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([f_sents[0], s_sents[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7w/lj581pkd4_95yc_rm7yk7z4h0000gn/T/ipykernel_93752/535803318.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fc1 = self.softmax(fc1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0832, 0.9168]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_similarity(f_sent, s_sent):\n",
    "        f_sent = tokenizer(f_sent, max_length=20, padding='max_length', return_tensors='pt')\n",
    "        s_sent = tokenizer(s_sent, max_length=20, padding='max_length', return_tensors='pt')\n",
    "        return model([f_sent, s_sent])\n",
    "\n",
    "\n",
    "f_sent = \"Нарисуй Альберта Эйнштейна в стиле Ван Гога\"\n",
    "s_sent = \"Нарисуй изображение\"\n",
    "\n",
    "\n",
    "get_similarity(f_sent, s_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'siamese_state')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
