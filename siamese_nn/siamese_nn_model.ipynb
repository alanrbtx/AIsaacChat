{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alanbarsag/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, BertModel\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'cointegrated/rubert-tiny'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = BertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('siamese_dataset', 'r') as f:\n",
    "    text = f.readlines()\n",
    "    text = text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('siamese_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper processing from text file\n",
    "raw_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#long processing from the text file\n",
    "first_column = []\n",
    "for i in text:\n",
    "    test = i.split(',')\n",
    "    test[0] = test[0].replace('\"', '')\n",
    "    first_column.append(test[0])\n",
    "\n",
    "second_column = []\n",
    "for i in text:\n",
    "    test = i.split(',')\n",
    "    test[1] = test[1].replace('\"', '')\n",
    "    second_column.append(test[1])\n",
    "\n",
    "labels_column = []\n",
    "for i in text:\n",
    "    test = i.split(',')\n",
    "    test[2] = test[2].replace('\"', '')\n",
    "    test[2] = test[2].replace('\\n', '')\n",
    "    test[2] = int(test[2])\n",
    "    labels_column.append(test[2])\n",
    "\n",
    "df = pd.DataFrame({'first': first_column, 'second': second_column, 'labels': labels_column})\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['first'], example['second'])\n",
    "\n",
    "dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.remove_columns(['first', 'second'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super().__init__()\n",
    "        self.lambd = lambd\n",
    "    \n",
    "    def forward(self, x):\n",
    "         return self.lambd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNN, self).__init__()\n",
    "        l1_norm = lambda x: 1 - torch.abs(x[0] - x[1])\n",
    "        self.encoder = BertModel.from_pretrained(checkpoint)\n",
    "        self.merged = Lambda(l1_norm)\n",
    "        self.fc1 = torch.nn.Linear(312, 2)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        first_encoded = self.encoder(**x[0]).pooler_output\n",
    "        #print(\"First: \", first_encoded)\n",
    "        second_encoded = self.encoder(**x[1]).pooler_output\n",
    "        l1_distance = self.merged([first_encoded, second_encoded])\n",
    "        #print(l1_distance.shape)\n",
    "        fc1 = self.fc1(l1_distance)\n",
    "        fc1 = self.softmax(fc1)\n",
    "        return fc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sent_tokenized = tokenizer(first_column, max_length=20, padding='max_length', truncation=True, return_tensors='pt')\n",
    "second_sent_tokenized = tokenizer(first_column, max_length=20, padding='max_length', truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_number = len(first_sent_tokenized['input_ids'])//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sent_raw_batches = [raw_dataset['first'][16*i: 16*(i+1)] for i in range(batch_number)]\n",
    "s_sent_raw_batches = [raw_dataset['second'][16*i: 16*(i+1)] for i in range(batch_number)]\n",
    "labels_batches = [labels_column[16*i: 16*(i+1)] for i in range(batch_number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sent_tokenized_batches = [tokenizer(batch, max_length=20, padding='max_length', truncation=True, return_tensors='pt') for batch in f_sent_raw_batches]\n",
    "s_sent_tokenized_batches = [tokenizer(batch, max_length=20, padding='max_length', truncation=True, return_tensors='pt') for batch in s_sent_raw_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 13, 3, 5, 10, 2, 4, 9, 6, 1, 15, 7, 16, 0, 12, 8, 17, 11]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [i for i in range(batch_number)]\n",
    "random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sent_shuffled = []\n",
    "s_sent_shuffled = []\n",
    "labels_shuffled = []\n",
    "\n",
    "for i in idx:\n",
    "    f_sent_shuffled.append(f_sent_tokenized_batches[i])\n",
    "    s_sent_shuffled.append(s_sent_tokenized_batches[i])\n",
    "    labels_shuffled.append(labels_batches[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7w/lj581pkd4_95yc_rm7yk7z4h0000gn/T/ipykernel_71086/535803318.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fc1 = self.softmax(fc1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9397059082984924\n",
      "0.9196892976760864\n",
      "0.9065229296684265\n",
      "0.9207167029380798\n",
      "0.9066671133041382\n",
      "0.8932352066040039\n",
      "0.8718301057815552\n",
      "0.8498995900154114\n",
      "0.7771111726760864\n",
      "0.5950617790222168\n",
      "0.6093410849571228\n",
      "0.60396808385849\n",
      "0.6023807525634766\n",
      "0.5955376625061035\n",
      "0.587152898311615\n",
      "0.553960919380188\n",
      "0.5671082735061646\n",
      "0.5453800559043884\n",
      "0.8297969698905945\n",
      "0.8353330492973328\n",
      "0.8189462423324585\n",
      "0.7687628269195557\n",
      "0.7558953762054443\n",
      "0.7529784440994263\n",
      "0.7450742125511169\n",
      "0.7289241552352905\n",
      "0.6758740544319153\n",
      "0.5421608686447144\n",
      "0.5407372117042542\n",
      "0.5329168438911438\n",
      "0.5207831859588623\n",
      "0.5119249224662781\n",
      "0.5083759427070618\n",
      "0.5344517827033997\n",
      "0.5161306262016296\n",
      "0.5303639769554138\n",
      "0.8851749897003174\n",
      "0.8798651695251465\n",
      "0.7489895224571228\n",
      "0.6072186827659607\n",
      "0.5973057746887207\n",
      "0.5865492820739746\n",
      "0.5790407657623291\n",
      "0.562112033367157\n",
      "0.5325314998626709\n",
      "0.49003252387046814\n",
      "0.48698562383651733\n",
      "0.4844975173473358\n",
      "0.4781672954559326\n",
      "0.4698370099067688\n",
      "0.46951255202293396\n",
      "0.5170429348945618\n",
      "0.48900535702705383\n",
      "0.5031857490539551\n",
      "0.8728959560394287\n",
      "0.8624287843704224\n",
      "0.6529961228370667\n",
      "0.4512718915939331\n",
      "0.44874030351638794\n",
      "0.441936731338501\n",
      "0.43597790598869324\n",
      "0.42916494607925415\n",
      "0.427627295255661\n",
      "0.46347299218177795\n",
      "0.4632938504219055\n",
      "0.4564969837665558\n",
      "0.44593140482902527\n",
      "0.43681052327156067\n",
      "0.4292072653770447\n",
      "0.47624483704566956\n",
      "0.4526093006134033\n",
      "0.46259599924087524\n",
      "0.8475133776664734\n",
      "0.8350092768669128\n",
      "0.5926046371459961\n",
      "0.38253253698349\n",
      "0.38211125135421753\n",
      "0.37891900539398193\n",
      "0.37644848227500916\n",
      "0.3738727271556854\n",
      "0.37632179260253906\n",
      "0.4097956717014313\n",
      "0.4106682538986206\n",
      "0.4052734971046448\n",
      "0.39962679147720337\n",
      "0.3915511965751648\n",
      "0.38826513290405273\n",
      "0.43648338317871094\n",
      "0.41194435954093933\n",
      "0.4172274172306061\n",
      "0.7838732600212097\n",
      "0.7633601427078247\n",
      "0.5275970697402954\n",
      "0.35534948110580444\n",
      "0.35569995641708374\n",
      "0.3543368875980377\n",
      "0.35241472721099854\n",
      "0.3514809310436249\n",
      "0.35447677969932556\n",
      "0.3812578022480011\n",
      "0.3812839984893799\n",
      "0.3774583339691162\n",
      "0.3721352815628052\n",
      "0.3664705157279968\n",
      "0.3636202812194824\n",
      "0.39505407214164734\n",
      "0.3791966438293457\n",
      "0.3807879686355591\n",
      "0.6613393425941467\n",
      "0.63548344373703\n",
      "0.44722071290016174\n",
      "0.34258195757865906\n",
      "0.34320488572120667\n",
      "0.3424069285392761\n",
      "0.3416052460670471\n",
      "0.34111925959587097\n",
      "0.3428806662559509\n",
      "0.35979026556015015\n",
      "0.3613489866256714\n",
      "0.35793599486351013\n",
      "0.35331180691719055\n",
      "0.3493732213973999\n",
      "0.3475096523761749\n",
      "0.36742642521858215\n",
      "0.3571677803993225\n",
      "0.35428744554519653\n",
      "0.551966667175293\n",
      "0.5267651081085205\n",
      "0.39246666431427\n",
      "0.3356143832206726\n",
      "0.3360483944416046\n",
      "0.3355758488178253\n",
      "0.3349872827529907\n",
      "0.33466196060180664\n",
      "0.3354645073413849\n",
      "0.34413522481918335\n",
      "0.34527307748794556\n",
      "0.3428245782852173\n",
      "0.33989790081977844\n",
      "0.33752939105033875\n",
      "0.3366459012031555\n",
      "0.34715813398361206\n",
      "0.342753529548645\n",
      "0.33892691135406494\n",
      "0.45375561714172363\n",
      "0.4368271827697754\n",
      "0.36130744218826294\n",
      "0.3310302197933197\n",
      "0.33132269978523254\n",
      "0.3310791552066803\n",
      "0.3305341601371765\n",
      "0.330335408449173\n",
      "0.33088329434394836\n",
      "0.3379661440849304\n",
      "0.3390951156616211\n",
      "0.336956650018692\n",
      "0.3341887593269348\n",
      "0.33246880769729614\n",
      "0.33152633905410767\n",
      "0.33796581625938416\n",
      "0.33557385206222534\n",
      "0.3310858905315399\n",
      "0.38588908314704895\n",
      "0.38272204995155334\n",
      "0.3456815779209137\n",
      "0.32754138112068176\n",
      "0.327718049287796\n",
      "0.32759717106819153\n",
      "0.3271193206310272\n",
      "0.32695555686950684\n",
      "0.32705092430114746\n",
      "0.32927829027175903\n",
      "0.32981768250465393\n",
      "0.32847434282302856\n",
      "0.32704299688339233\n",
      "0.32598042488098145\n",
      "0.32564765214920044\n",
      "0.3284633755683899\n",
      "0.32725778222084045\n",
      "0.32484936714172363\n",
      "0.363734632730484\n",
      "0.3619135618209839\n",
      "0.33723339438438416\n",
      "0.3248838782310486\n",
      "0.32504594326019287\n",
      "0.32500290870666504\n",
      "0.32455798983573914\n",
      "0.3244756758213043\n",
      "0.3245171308517456\n",
      "0.3261001706123352\n",
      "0.32662853598594666\n",
      "0.32575201988220215\n",
      "0.3245071768760681\n",
      "0.3238145112991333\n",
      "0.3234933614730835\n",
      "0.3254697918891907\n",
      "0.32472243905067444\n",
      "0.32254159450531006\n",
      "0.3476336896419525\n",
      "0.34746891260147095\n",
      "0.331615686416626\n",
      "0.3228764832019806\n",
      "0.32300323247909546\n",
      "0.3230109214782715\n",
      "0.3226267695426941\n",
      "0.32256370782852173\n",
      "0.3225111663341522\n",
      "0.3228362500667572\n",
      "0.3231510818004608\n",
      "0.32251980900764465\n",
      "0.32178032398223877\n",
      "0.3212517201900482\n",
      "0.32111603021621704\n",
      "0.3219752013683319\n",
      "0.32145002484321594\n",
      "0.3201977610588074\n",
      "0.34214162826538086\n",
      "0.3412483036518097\n",
      "0.3283752501010895\n",
      "0.3214053809642792\n",
      "0.32151126861572266\n",
      "0.3215464949607849\n",
      "0.3212072551250458\n",
      "0.3211706280708313\n",
      "0.321144163608551\n",
      "0.3215824365615845\n",
      "0.3218928873538971\n",
      "0.3214437663555145\n",
      "0.32073330879211426\n",
      "0.3203412890434265\n",
      "0.3201843798160553\n",
      "0.3209545910358429\n",
      "0.32057684659957886\n",
      "0.31935349106788635\n",
      "0.3353736400604248\n",
      "0.33502596616744995\n",
      "0.32570281624794006\n",
      "0.32026490569114685\n",
      "0.32035142183303833\n",
      "0.3204011023044586\n",
      "0.32010403275489807\n",
      "0.32007846236228943\n",
      "0.32002580165863037\n",
      "0.3200664520263672\n",
      "0.32028645277023315\n",
      "0.3198971748352051\n",
      "0.31939804553985596\n",
      "0.31908002495765686\n",
      "0.31897950172424316\n",
      "0.31932270526885986\n",
      "0.31901875138282776\n",
      "0.3181864619255066\n",
      "0.33286765217781067\n",
      "0.3322095572948456\n",
      "0.324070006608963\n",
      "0.31939443945884705\n",
      "0.3194641172885895\n",
      "0.3195227384567261\n",
      "0.3192625641822815\n",
      "0.31924545764923096\n",
      "0.31920841336250305\n",
      "0.3193146288394928\n",
      "0.3195268213748932\n",
      "0.31922319531440735\n",
      "0.3187580108642578\n",
      "0.31850382685661316\n",
      "0.3184027373790741\n",
      "0.31872397661209106\n",
      "0.3184872567653656\n",
      "0.31769850850105286\n",
      "0.32943373918533325\n",
      "0.3290366530418396\n",
      "0.3225795030593872\n",
      "0.3186912536621094\n",
      "0.31874528527259827\n",
      "0.31881004571914673\n",
      "0.31857937574386597\n",
      "0.31856662034988403\n",
      "0.3185183107852936\n",
      "0.31846898794174194\n",
      "0.3186335861682892\n",
      "0.31836622953414917\n",
      "0.3179980218410492\n",
      "0.3177887797355652\n",
      "0.3177052438259125\n",
      "0.31784486770629883\n",
      "0.31764331459999084\n",
      "0.31704095005989075\n",
      "0.3279067575931549\n",
      "0.32738715410232544\n",
      "0.32158151268959045\n",
      "0.3181244730949402\n",
      "0.318168044090271\n",
      "0.31823527812957764\n",
      "0.31802812218666077\n",
      "0.31801944971084595\n",
      "0.31797951459884644\n",
      "0.3179663419723511\n",
      "0.31812363862991333\n",
      "0.3179037868976593\n",
      "0.31756243109703064\n",
      "0.3173866868019104\n",
      "0.3173070549964905\n",
      "0.3174365162849426\n",
      "0.3172702193260193\n",
      "0.31670770049095154\n",
      "0.3259330093860626\n",
      "0.32552650570869446\n",
      "0.32066136598587036\n",
      "0.3176521062850952\n",
      "0.31768789887428284\n",
      "0.31775522232055664\n",
      "0.3175695240497589\n",
      "0.3175615072250366\n",
      "0.317518025636673\n",
      "0.3174363970756531\n",
      "0.31756773591041565\n",
      "0.31737038493156433\n",
      "0.3170822262763977\n",
      "0.31693264842033386\n",
      "0.31686368584632874\n",
      "0.31690388917922974\n",
      "0.3167586028575897\n",
      "0.31629857420921326\n",
      "0.32486844062805176\n",
      "0.3244122564792633\n",
      "0.3199954926967621\n",
      "0.3172580897808075\n",
      "0.31728628277778625\n",
      "0.31735390424728394\n",
      "0.31718647480010986\n",
      "0.31717899441719055\n",
      "0.3171410858631134\n",
      "0.31707218289375305\n",
      "0.31719711422920227\n",
      "0.3170274794101715\n",
      "0.3167628347873688\n",
      "0.31663310527801514\n",
      "0.3165689706802368\n",
      "0.3165985345840454\n",
      "0.3164735436439514\n",
      "0.316050261259079\n",
      "0.323633074760437\n",
      "0.323232501745224\n",
      "0.3193743824958801\n",
      "0.3169223964214325\n",
      "0.3169444799423218\n",
      "0.3170109987258911\n",
      "0.3168589174747467\n",
      "0.3168511688709259\n",
      "0.3168144226074219\n",
      "0.3167164921760559\n",
      "0.31682777404785156\n",
      "0.31667205691337585\n",
      "0.31644079089164734\n",
      "0.31632551550865173\n",
      "0.31626808643341064\n",
      "0.3162585496902466\n",
      "0.3161470890045166\n",
      "0.3157825171947479\n",
      "0.32281363010406494\n",
      "0.322418212890625\n",
      "0.3188873529434204\n",
      "0.3166347146034241\n",
      "0.3166515827178955\n",
      "0.3167167901992798\n",
      "0.31657829880714417\n",
      "0.3165702819824219\n",
      "0.3165373206138611\n",
      "0.31644201278686523\n",
      "0.3165469169616699\n",
      "0.31640738248825073\n",
      "0.31619665026664734\n",
      "0.3160937428474426\n",
      "0.3160405158996582\n",
      "0.3160223066806793\n",
      "0.3159244656562805\n",
      "0.3155907392501831\n",
      "0.3219764530658722\n",
      "0.32162562012672424\n",
      "0.318443238735199\n",
      "0.3163847029209137\n",
      "0.31639719009399414\n",
      "0.31646066904067993\n",
      "0.3163341283798218\n",
      "0.31632551550865173\n",
      "0.31629398465156555\n",
      "0.3161871135234833\n",
      "0.31628280878067017\n",
      "0.31615379452705383\n",
      "0.31596577167510986\n",
      "0.3158727288246155\n",
      "0.31582388281822205\n",
      "0.31578728556632996\n",
      "0.31569963693618774\n",
      "0.31540271639823914\n",
      "0.32134389877319336\n",
      "0.3210110664367676\n",
      "0.31807464361190796\n",
      "0.31616610288619995\n",
      "0.31617534160614014\n",
      "0.3162364959716797\n",
      "0.3161204755306244\n",
      "0.31611162424087524\n",
      "0.31608226895332336\n",
      "0.3159771263599396\n",
      "0.31606701016426086\n",
      "0.315949410200119\n",
      "0.3157769739627838\n",
      "0.31569281220436096\n",
      "0.3156470060348511\n",
      "0.3156050741672516\n",
      "0.31552624702453613\n",
      "0.31525295972824097\n",
      "0.3207387328147888\n",
      "0.3204406201839447\n",
      "0.3177434504032135\n",
      "0.31597307324409485\n",
      "0.31597933173179626\n",
      "0.3160383403301239\n",
      "0.31593188643455505\n",
      "0.31592270731925964\n",
      "0.3158937096595764\n",
      "0.3157809376716614\n",
      "0.31586435437202454\n",
      "0.31575536727905273\n",
      "0.31559959053993225\n",
      "0.3155229985713959\n",
      "0.3154807984828949\n",
      "0.31542766094207764\n",
      "0.31535640358924866\n",
      "0.31510910391807556\n",
      "0.32025259733200073\n",
      "0.31997156143188477\n",
      "0.3174566328525543\n",
      "0.31580162048339844\n",
      "0.3158053159713745\n",
      "0.31586211919784546\n",
      "0.31576424837112427\n",
      "0.3157549500465393\n",
      "0.3157274127006531\n",
      "0.3156190514564514\n",
      "0.31569740176200867\n",
      "0.31559765338897705\n",
      "0.31545311212539673\n",
      "0.3153828978538513\n",
      "0.3153431713581085\n",
      "0.31528961658477783\n",
      "0.31522494554519653\n",
      "0.31499481201171875\n",
      "0.3197774589061737\n",
      "0.3195257782936096\n",
      "0.31719380617141724\n",
      "0.315647691488266\n",
      "0.3156495988368988\n",
      "0.3157043159008026\n",
      "0.31561416387557983\n",
      "0.3156048059463501\n",
      "0.3155770003795624\n",
      "0.31546372175216675\n",
      "0.3155363202095032\n",
      "0.3154430687427521\n",
      "0.31531134247779846\n",
      "0.31524670124053955\n",
      "0.31520983576774597\n",
      "0.3151504099369049\n",
      "0.31509074568748474\n",
      "0.3148806095123291\n",
      "0.3193981647491455\n",
      "0.3191578686237335\n",
      "0.31696584820747375\n",
      "0.31550899147987366\n",
      "0.31550922989845276\n",
      "0.3155621886253357\n",
      "0.3154788613319397\n",
      "0.3154697120189667\n",
      "0.3154430389404297\n",
      "0.31533485651016235\n",
      "0.31540343165397644\n",
      "0.31531739234924316\n",
      "0.31519457697868347\n",
      "0.31513482332229614\n",
      "0.3150998651981354\n",
      "0.3150424659252167\n",
      "0.31498730182647705\n",
      "0.3147902190685272\n",
      "0.3190113604068756\n",
      "0.3187960684299469\n",
      "0.3167492747306824\n",
      "0.31538328528404236\n",
      "0.3153822124004364\n",
      "0.3154333531856537\n",
      "0.31535613536834717\n",
      "0.31534725427627563\n",
      "0.3153204619884491\n",
      "0.3152086138725281\n",
      "0.315272718667984\n",
      "0.31519150733947754\n",
      "0.31507933139801025\n",
      "0.31502389907836914\n",
      "0.31499138474464417\n",
      "0.31493034958839417\n",
      "0.31487882137298584\n",
      "0.3146982491016388\n",
      "0.3186996281147003\n",
      "0.3184928297996521\n",
      "0.31655797362327576\n",
      "0.3152691721916199\n",
      "0.31526726484298706\n",
      "0.3153163492679596\n",
      "0.3152444660663605\n",
      "0.31523597240448\n",
      "0.31521010398864746\n",
      "0.3151033818721771\n",
      "0.31516411900520325\n",
      "0.31508868932724\n",
      "0.31498366594314575\n",
      "0.31493210792541504\n",
      "0.3149011731147766\n",
      "0.3148419260978699\n",
      "0.3147931396961212\n",
      "0.3146236836910248\n",
      "0.3183862864971161\n",
      "0.3181958496570587\n",
      "0.316377729177475\n",
      "0.31516504287719727\n",
      "0.3151625096797943\n",
      "0.3152095675468445\n",
      "0.31514251232147217\n",
      "0.3151341676712036\n",
      "0.31510865688323975\n",
      "0.31500184535980225\n",
      "0.3150590658187866\n",
      "0.31498757004737854\n",
      "0.31489062309265137\n",
      "0.3148423731327057\n",
      "0.3148132562637329\n",
      "0.3147527873516083\n",
      "0.31470635533332825\n",
      "0.3145495653152466\n",
      "0.3181256353855133\n",
      "0.3179411292076111\n",
      "0.3162144124507904\n",
      "0.31506964564323425\n",
      "0.3150668144226074\n",
      "0.3151119649410248\n",
      "0.31504935026168823\n",
      "0.3150411546230316\n",
      "0.3150167167186737\n",
      "0.3149144649505615\n",
      "0.31496894359588623\n",
      "0.3149018883705139\n",
      "0.3148109018802643\n",
      "0.31476569175720215\n",
      "0.3147379457950592\n",
      "0.3146798014640808\n",
      "0.31463563442230225\n",
      "0.3144875764846802\n",
      "0.31786903738975525\n",
      "0.3176911175251007\n",
      "0.3160611093044281\n",
      "0.31498193740844727\n",
      "0.3149789273738861\n",
      "0.31502220034599304\n",
      "0.31496378779411316\n",
      "0.31495559215545654\n",
      "0.31493133306503296\n",
      "0.3148283064365387\n",
      "0.31487977504730225\n",
      "0.314815878868103\n",
      "0.31473201513290405\n",
      "0.3146894872188568\n",
      "0.3146636486053467\n",
      "0.3146045506000519\n",
      "0.3145628273487091\n",
      "0.31442517042160034\n",
      "0.3176511824131012\n",
      "0.3174722194671631\n",
      "0.3159211277961731\n",
      "0.31490111351013184\n",
      "0.3148978054523468\n",
      "0.31493932008743286\n",
      "0.31488457322120667\n",
      "0.31487640738487244\n",
      "0.3148534595966339\n",
      "0.3147558569908142\n",
      "0.3148050308227539\n",
      "0.31474462151527405\n",
      "0.31466561555862427\n",
      "0.31462565064430237\n",
      "0.31460079550743103\n",
      "0.31454402208328247\n",
      "0.3145042955875397\n",
      "0.3143736720085144\n",
      "0.317432165145874\n",
      "0.31725871562957764\n",
      "0.31579074263572693\n",
      "0.3148263096809387\n",
      "0.3148230016231537\n",
      "0.31486275792121887\n",
      "0.3148113191127777\n",
      "0.3148033618927002\n",
      "0.31478074193000793\n",
      "0.3146827220916748\n",
      "0.3147292733192444\n",
      "0.3146713674068451\n",
      "0.31459829211235046\n",
      "0.3145604431629181\n",
      "0.314536988735199\n",
      "0.3144793212413788\n",
      "0.314441442489624\n",
      "0.3143196702003479\n",
      "0.3172520399093628\n",
      "0.31707754731178284\n",
      "0.31567466259002686\n",
      "0.3147572875022888\n",
      "0.31475383043289185\n",
      "0.31479206681251526\n",
      "0.31474366784095764\n",
      "0.3147357702255249\n",
      "0.3147141635417938\n",
      "0.3146207928657532\n",
      "0.31466543674468994\n",
      "0.3146105706691742\n",
      "0.3145415484905243\n",
      "0.3145058751106262\n",
      "0.3144833743572235\n",
      "0.3144281506538391\n",
      "0.31439194083213806\n",
      "0.31427595019340515\n",
      "0.317063570022583\n",
      "0.3168932795524597\n",
      "0.315563440322876\n",
      "0.3146931231021881\n",
      "0.31468960642814636\n",
      "0.314726322889328\n",
      "0.314680814743042\n",
      "0.3146730065345764\n",
      "0.3146519362926483\n",
      "0.31455928087234497\n",
      "0.31460174918174744\n",
      "0.31454893946647644\n",
      "0.3144845962524414\n",
      "0.3144506514072418\n",
      "0.31442922353744507\n",
      "0.31437402963638306\n",
      "0.31433919072151184\n",
      "0.3142303228378296\n",
      "0.31690555810928345\n",
      "0.3167364001274109\n",
      "0.3154643774032593\n",
      "0.31463342905044556\n",
      "0.31463000178337097\n",
      "0.31466537714004517\n",
      "0.3146224617958069\n",
      "0.31461483240127563\n",
      "0.3145942986011505\n",
      "0.31450411677360535\n",
      "0.3145447373390198\n",
      "0.3144942820072174\n",
      "0.3144337236881256\n",
      "0.3144015371799469\n",
      "0.3143808841705322\n",
      "0.3143273890018463\n",
      "0.31429392099380493\n",
      "0.31419065594673157\n",
      "0.31674984097480774\n",
      "0.31658437848091125\n",
      "0.31537115573883057\n",
      "0.31457799673080444\n",
      "0.31457459926605225\n",
      "0.31460854411125183\n",
      "0.3145681619644165\n",
      "0.31456059217453003\n",
      "0.3145405054092407\n",
      "0.3144514262676239\n",
      "0.31449025869369507\n",
      "0.31444185972213745\n",
      "0.3143850564956665\n",
      "0.31435438990592957\n",
      "0.3143347203731537\n",
      "0.314282089471817\n",
      "0.3142498731613159\n",
      "0.3141523003578186\n",
      "0.31660938262939453\n",
      "0.3164461851119995\n",
      "0.3152851164340973\n",
      "0.31452614068984985\n",
      "0.31452277302742004\n",
      "0.31455543637275696\n",
      "0.3145173192024231\n",
      "0.3145098090171814\n",
      "0.3144903779029846\n",
      "0.31440433859825134\n",
      "0.3144415020942688\n",
      "0.3143952786922455\n",
      "0.31434139609336853\n",
      "0.3143121600151062\n",
      "0.31429323554039\n",
      "0.31424203515052795\n",
      "0.3142111301422119\n",
      "0.3141182065010071\n",
      "0.31647247076034546\n",
      "0.316312700510025\n",
      "0.3152044415473938\n",
      "0.31447750329971313\n",
      "0.3144741654396057\n",
      "0.31450551748275757\n",
      "0.31446948647499084\n",
      "0.3144620954990387\n",
      "0.31444311141967773\n",
      "0.31435859203338623\n",
      "0.3143942356109619\n",
      "0.31434980034828186\n",
      "0.314299076795578\n",
      "0.31427106261253357\n",
      "0.314253032207489\n",
      "0.31420260667800903\n",
      "0.31417298316955566\n",
      "0.3140847980976105\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for s_batch, f_batch, target in zip(f_sent_tokenized_batches, s_sent_tokenized_batches, labels_batches):\n",
    "        output = model([s_batch, f_batch])\n",
    "        loss = loss_fn(output, torch.tensor(target))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7w/lj581pkd4_95yc_rm7yk7z4h0000gn/T/ipykernel_71086/535803318.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fc1 = self.softmax(fc1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0049, 0.9951],\n",
       "        [0.0061, 0.9939],\n",
       "        [0.0083, 0.9917],\n",
       "        [0.0053, 0.9947],\n",
       "        [0.0062, 0.9938],\n",
       "        [0.0063, 0.9937],\n",
       "        [0.0053, 0.9947],\n",
       "        [0.0076, 0.9924],\n",
       "        [0.0055, 0.9945],\n",
       "        [0.0042, 0.9958],\n",
       "        [0.0086, 0.9914],\n",
       "        [0.0044, 0.9956],\n",
       "        [0.0051, 0.9949],\n",
       "        [0.0043, 0.9957],\n",
       "        [0.0044, 0.9956],\n",
       "        [0.0048, 0.9952]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([f_sent_tokenized_batches[0], s_sent_tokenized_batches[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7w/lj581pkd4_95yc_rm7yk7z4h0000gn/T/ipykernel_71086/535803318.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fc1 = self.softmax(fc1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0112, 0.9888]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_sent = \"Нарисуй Альберта Эйнштейна в стиле Ван Гога\"\n",
    "s_sent = \"Нарисуй изображение\"\n",
    "\n",
    "f_sent = tokenizer(f_sent, max_length=20, padding='max_length', return_tensors='pt')\n",
    "s_sent = tokenizer(s_sent, max_length=20, padding='max_length', return_tensors='pt')\n",
    "\n",
    "model([f_sent, s_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'siamese_state')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
