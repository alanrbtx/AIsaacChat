{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:21:28.451392: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-03 12:21:28.905844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2023-06-03 12:21:28.905895: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2023-06-03 12:21:28.905900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM, MarianMTModel, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, GPT2Tokenizer\n",
    "from datasets import  Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import torch, gc\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('ai-forever/ruT5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('ai-forever/ruT5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: ru_turbo_alpaca/default\n",
      "Found cached dataset ru_turbo_alpaca (/home/alan-robotics/.cache/huggingface/datasets/IlyaGusev___ru_turbo_alpaca/default/0.0.1/a2a1f5b065b9e34022f6bc402785c2f5fa791930917ce4f1b8d4e634def7496d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad43c32507a480a9e1c60750f329d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset('IlyaGusev/ru_turbo_alpaca')\n",
    "\n",
    "raw_dataset = raw_dataset.remove_columns(['input', 'alternative_output', 'label', 'all_labels', 'agreement', 'overlap'])\n",
    "raw_dataset = raw_dataset['train'].train_test_split(0.2, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab70f3b698d340cabe3329e2c22ba52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1105d56eef6c44d7a3884042f291918e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenized_function(example):\n",
    "    tokenized_snippet = tokenizer(example['output'], truncation=True, max_length=512)\n",
    "    example['labels'] = tokenized_snippet['input_ids']\n",
    "    return tokenizer(example['instruction'], truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "dataset = raw_dataset.map(tokenized_function, batched=True)\n",
    "\n",
    "dataset = dataset.remove_columns(['instruction', 'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    #prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    #result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    #{k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = \"ruT5-trainer\"\n",
    "os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_train_epochs = 2\n",
    "logging_steps = len(dataset['train']) // batch_size\n",
    "\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir='ruT5-finetuned',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True\n",
    "    )\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator,\n",
    "    dataset['train'],\n",
    "    dataset['test'],\n",
    "    tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaf807db4d149828fb0a7e33cc8d5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3775, 'learning_rate': 0.0, 'epoch': 0.08}\n",
      "{'loss': 2.3516, 'learning_rate': 0.0, 'epoch': 0.17}\n",
      "{'loss': 2.3957, 'learning_rate': 0.0, 'epoch': 0.25}\n",
      "{'loss': 2.3747, 'learning_rate': 0.0, 'epoch': 0.34}\n",
      "{'loss': 2.3941, 'learning_rate': 0.0, 'epoch': 0.42}\n",
      "{'loss': 2.4017, 'learning_rate': 0.0, 'epoch': 0.5}\n",
      "{'loss': 2.3643, 'learning_rate': 0.0, 'epoch': 0.59}\n",
      "{'loss': 2.3541, 'learning_rate': 0.0, 'epoch': 0.67}\n",
      "{'loss': 2.3853, 'learning_rate': 0.0, 'epoch': 0.75}\n",
      "{'loss': 2.4126, 'learning_rate': 0.0, 'epoch': 0.84}\n",
      "{'loss': 2.3964, 'learning_rate': 0.0, 'epoch': 0.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef08dbcf4b754e88b6e80e88c118c086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.4204769548268746, 0.16815782024933237, 0.09123486939902314, 0.053833417320567276]\" of type <class 'list'> for key \"eval_precisions\" as a metric. MLflow's log_metric() only accepts float and int types so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.4204769548268746, 0.16815782024933237, 0.09123486939902314, 0.053833417320567276]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1888267993927, 'eval_bleu': 0.012522647648255015, 'eval_precisions': [0.4204769548268746, 0.16815782024933237, 0.09123486939902314, 0.053833417320567276], 'eval_brevity_penalty': 0.09173353783512943, 'eval_length_ratio': 0.2950838529384898, 'eval_translation_length': 87220, 'eval_reference_length': 295577, 'eval_runtime': 279.6369, 'eval_samples_per_second': 21.331, 'eval_steps_per_second': 5.335, 'epoch': 1.0}\n",
      "{'loss': 2.3695, 'learning_rate': 0.0, 'epoch': 1.01}\n",
      "{'loss': 2.3983, 'learning_rate': 0.0, 'epoch': 1.09}\n",
      "{'loss': 2.3917, 'learning_rate': 0.0, 'epoch': 1.17}\n",
      "{'loss': 2.4065, 'learning_rate': 0.0, 'epoch': 1.26}\n",
      "{'loss': 2.4005, 'learning_rate': 0.0, 'epoch': 1.34}\n",
      "{'loss': 2.377, 'learning_rate': 0.0, 'epoch': 1.42}\n",
      "{'loss': 2.3727, 'learning_rate': 0.0, 'epoch': 1.51}\n",
      "{'loss': 2.3663, 'learning_rate': 0.0, 'epoch': 1.59}\n",
      "{'loss': 2.3635, 'learning_rate': 0.0, 'epoch': 1.68}\n",
      "{'loss': 2.3841, 'learning_rate': 0.0, 'epoch': 1.76}\n",
      "{'loss': 2.3749, 'learning_rate': 0.0, 'epoch': 1.84}\n",
      "{'loss': 2.3782, 'learning_rate': 0.0, 'epoch': 1.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9abdc734a8b426f89a9140e841cca7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.4204769548268746, 0.16815782024933237, 0.09123486939902314, 0.053833417320567276]\" of type <class 'list'> for key \"eval_precisions\" as a metric. MLflow's log_metric() only accepts float and int types so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.4204769548268746, 0.16815782024933237, 0.09123486939902314, 0.053833417320567276]\" of type <class 'list'> for key \"eval/precisions\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1888267993927, 'eval_bleu': 0.012522647648255015, 'eval_precisions': [0.4204769548268746, 0.16815782024933237, 0.09123486939902314, 0.053833417320567276], 'eval_brevity_penalty': 0.09173353783512943, 'eval_length_ratio': 0.2950838529384898, 'eval_translation_length': 87220, 'eval_reference_length': 295577, 'eval_runtime': 275.6395, 'eval_samples_per_second': 21.641, 'eval_steps_per_second': 5.413, 'epoch': 2.0}\n",
      "{'train_runtime': 1696.2786, 'train_samples_per_second': 28.129, 'train_steps_per_second': 7.033, 'train_loss': 2.382459232813155, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11930, training_loss=2.382459232813155, metrics={'train_runtime': 1696.2786, 'train_samples_per_second': 28.129, 'train_steps_per_second': 7.033, 'train_loss': 2.382459232813155, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "git config --global user.email \"alanrbtx@gmail.com\"\n",
    "git config --global user.name \"alanrbtx\"\n",
    "git add mlruns\n",
    "git commit -m 'Add MLFlow run'\n",
    "git push https://github.com/alanrbtx/AIsaacChat.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для приготовления лазаньи вам понадобятся: - 2 яйца - 2 ст. ложки оливкового масла - 2 ст. ложки оливкового масла - 2 ст. ложки оливкового масла - 2 ст. ложки оливкового масла - 1 ст. ложка оливкового масла - 1 ст. ложка оливкового масла - 1 ст. ложка оливкового масла - 1 ст. ложка оливкового масла - 1 ст. ложка оливков\n"
     ]
    }
   ],
   "source": [
    "sentence = tokenizer('Как приготовить лазанью пошагово?', return_tensors='pt').to('cuda')\n",
    "\n",
    "res = model.generate(**sentence, max_length=100, early_stopping=True)\n",
    "print(tokenizer.decode(res[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('AlanRobotics/instruct-T5')\n",
    "tokenizer.push_to_hub('AlanRobotics/instruct-T5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
